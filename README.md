# Globant Data Engineering Challenge ðŸš€

Este proyecto resuelve el coding challenge propuesto por Globant para el rol de Data Engineer. Incluye un pipeline ETL desarrollado con PySpark, una API REST con FastAPI y pruebas automatizadas.

---

## Estructura del Proyecto

```
.
â”œâ”€â”€ api/                     # Endpoints y servicios de la API
â”œâ”€â”€ config/                  # ConfiguraciÃ³n de la DB (prod y test)
â”œâ”€â”€ data/uploads/            # Carpeta donde se suben los CSVs a cargar
â”œâ”€â”€ etl_outputs/             # Resultados del ETL (JSON)
â”œâ”€â”€ logs/                    # Logs rotativos de cada ejecuciÃ³n ETL
â”œâ”€â”€ queries_sql/             # Queries SQL centralizadas
â”œâ”€â”€ spark_jobs/              # Script PySpark de validaciÃ³n y carga
â”œâ”€â”€ tests/                   # Tests automÃ¡ticos con pytest
â”œâ”€â”€ Dockerfile               # Imagen Docker de FastAPI
â”œâ”€â”€ docker-compose.yml       # Contenedores: API + MySQL + Test DB
â”œâ”€â”€ main_api.py              # Punto de entrada de la API
â”œâ”€â”€ requirements.txt         # Dependencias del proyecto
```

---

## Setup local

### 1. Clonar repositorio

```bash
git clone <REPO_URL>
cd globant
```

### 2. Crear entorno virtual

```bash
python -m venv venv
venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux/Mac
```

### 3. Instalar dependencias

```bash
pip install -r requirements.txt
```

---

## Docker

### Levantar entorno completo (API + MySQL + TestDB)

```bash
docker-compose up -d
```

---

## API Endpoints (FastAPI)

Una vez levantada la API en `http://localhost:8000`, accede a la documentaciÃ³n en:

Swagger UI: [`http://localhost:8000/docs`](http://localhost:8000/docs)

### Endpoints principales:

- `POST /upload_csv/{table_name}` â†’ Subida de CSVs
- `POST /run_etl` â†’ Ejecuta el proceso ETL completo
- `GET /metrics/hires_per_quarter` â†’ Contrataciones por trimestre
- `GET /metrics/departments_above_avg` â†’ Departamentos con mÃ¡s contrataciones

---

## Proceso ETL (PySpark)

El proceso ETL hace lo siguiente:

1. Lee los archivos CSV cargados en `data/uploads/`
2. Valida duplicados, claves forÃ¡neas, campos nulos, etc.
3. Inserta los nuevos registros en MySQL
4. Guarda un `.json` con el resumen del batch
5. Genera un `.log` con el detalle de la ejecuciÃ³n

Se puede ejecutar manualmente:

```bash
spark-submit --jars mysql-connector-java-8.0.33.jar spark_jobs/validate_and_load.py
```

---

## Pruebas

Este proyecto incluye pruebas automatizadas:

```bash
set IS_TEST=1 && pytest
```

Cubre:

- Subida de archivos
- MÃ©tricas
- ETL completo (mock + real)
- Casos de error

---

##  Cloud-ready

Este proyecto estÃ¡ listo para deploy en cualquier plataforma cloud:

- **Base de datos:** Contenerizada
- **ETL:** Compatible con Spark local o cluster
- **API:** Dockerizada y portable
- **Logging:** Local + archivo
- **Pruebas:** Independientes, con base de datos de test

---

##  Bonus

- Batch ID Ãºnico por ejecuciÃ³n
- Rollback completo en caso de errores
- Logging detallado (`logs/`)
- Validaciones robustas por tabla
- ModularizaciÃ³n profesional de cÃ³digo

---

## ðŸ’¡ Autor

Desarrollado por Jonathan Villegas  â€” Abril 2025\
DesafÃ­o tÃ©cnico para Globant.

---

